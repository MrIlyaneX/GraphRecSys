{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader, LinkNeighborLoader\n",
    "import torch_geometric.transforms as T\n",
    "import torch\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = pd.read_csv(\"../data/anime.csv\")\n",
    "rating = pd.read_csv(\"../data/rating.csv\")\n",
    "\n",
    "anime[\"genre\"] = anime[\"genre\"].str.split(\", \")\n",
    "# anime = anime.dropna(axis=1)\n",
    "anime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = anime.dropna()\n",
    "anime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime[\"anime_id\"] = anime.loc[:, \"anime_id\"].apply(lambda x: x + 0.5)\n",
    "anime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = anime[\"type\"].unique()\n",
    "genres = anime[\"genre\"].explode().unique()\n",
    "\n",
    "type2id = {t: i for i, t in enumerate(types)}\n",
    "id2type = {i: t for i, t in enumerate(types)}\n",
    "\n",
    "genre2id = {g: i for i, g in enumerate(genres)}\n",
    "id2genre = {i: g for i, g in enumerate(genres)}\n",
    "\n",
    "\n",
    "unique_values = {\n",
    "    \"anime_id\": anime[\"anime_id\"].unique(),\n",
    "    \"types\": [type2id[t] for t in anime[\"type\"].unique()],\n",
    "    \"genre\": [genre2id[g] for g in anime[\"genre\"].explode().unique()],\n",
    "    \"user_id\": rating[\"user_id\"].unique(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "G.add_nodes_from(unique_values[\"anime_id\"], node_type=\"anime\")\n",
    "G.add_nodes_from(unique_values[\"types\"], node_type=\"types\")\n",
    "G.add_nodes_from(unique_values[\"genre\"], node_type=\"genre\")\n",
    "G.add_nodes_from(unique_values[\"user_id\"], node_type=\"user\")\n",
    "\n",
    "for anime_id in unique_values[\"anime_id\"]:\n",
    "    if G.nodes[anime_id][\"node_type\"] == \"anime\":\n",
    "        G.nodes[anime_id][\"rating\"] = anime[anime[\"anime_id\"] == anime_id][\n",
    "            \"rating\"\n",
    "        ].values\n",
    "\n",
    "for _, row in anime.iterrows():\n",
    "    anime_id = row[\"anime_id\"]\n",
    "\n",
    "    anime_type = type2id[row[\"type\"]]\n",
    "    genres = [genre2id[g] for g in row[\"genre\"]]\n",
    "\n",
    "    G.add_edge(anime_id, anime_type, relation=\"type\")\n",
    "\n",
    "    for genre in genres:\n",
    "        G.add_edge(anime_id, genre, relation=\"genre\")\n",
    "\n",
    "for _, row in rating.iterrows():\n",
    "    user_id = row[\"user_id\"]\n",
    "    anime_id = row[\"anime_id\"]\n",
    "    rating_value = row[\"rating\"]\n",
    "\n",
    "    G.add_edge(user_id, anime_id, weight=rating_value, relation=\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = []\n",
    "edge_type = []\n",
    "for u, v, data in G.edges(data=True):\n",
    "    edge_index.append([u, v])\n",
    "    if data[\"relation\"] == \"rating\":\n",
    "        edge_type.append(0)\n",
    "    elif data[\"relation\"] == \"type\":\n",
    "        edge_type.append(1)\n",
    "    elif data[\"relation\"] == \"genre\":\n",
    "        edge_type.append(2)\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).T\n",
    "edge_type = torch.tensor(edge_type, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = []\n",
    "for node in G:\n",
    "    node_type = G.nodes[node][\"node_type\"]\n",
    "    if node_type == \"anime\":\n",
    "        node_features.append(G.nodes[node][\"rating\"])\n",
    "    elif node_type == \"genre\":\n",
    "        node_features.append([0])\n",
    "    elif node_type == \"types\":\n",
    "        node_features.append([1])\n",
    "    elif node_type == \"user\":\n",
    "        node_features.append([2])\n",
    "\n",
    "x = torch.tensor(node_features, dtype=torch.float).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(x=x, edge_index=edge_index, edge_type=edge_type)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.is_directed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train, test, and validation sets on edge-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and split the data\n",
    "transforms = T.Compose(\n",
    "    [T.NormalizeFeatures(), T.RandomLinkSplit(num_val=0.1, num_test=0.2)]\n",
    ")\n",
    "\n",
    "train_data, val_data, test_data = transforms(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# Create DataLoaders for all sets of data\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    num_neighbors=[40, 40],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    edge_label_index=train_data.edge_index,\n",
    "    edge_label=train_data.edge_label,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = LinkNeighborLoader(\n",
    "    data=test_data,\n",
    "    num_neighbors=[40, 40],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    edge_label_index=test_data.edge_index,\n",
    "    edge_label=test_data.edge_label,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "val_loader = LinkNeighborLoader(\n",
    "    data=val_data,\n",
    "    num_neighbors=[40, 40],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    edge_label_index=val_data.edge_index,\n",
    "    edge_label=val_data.edge_label,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train DataLoader length: {len(train_loader)}\")\n",
    "print(f\"Test DataLoader length: {len(test_loader)}\")\n",
    "print(f\"Val DataLoader length: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of batch in train DataLoader\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save everything into the \".pkl\" file (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    \"train_loader\": train_loader,\n",
    "    \"test_loader\": test_loader,\n",
    "    \"val_loader\": val_loader,\n",
    "    \"data\": data,\n",
    "    \"graph\": G,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_path = \"../data/pickle_checkpoints/data_stats_v1.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_file_path, \"wb\") as file:\n",
    "    pickle.dump(data_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_file_path, \"rb\") as f:\n",
    "    data_dict = pickle.load(f)\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, edge_index, edge_type, edge_label = next(iter(train_loader))\n",
    "b = next(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = Data(\n",
    "    x=[10103, 1],\n",
    "    edge_index=[2, 78764],\n",
    "    edge_type=[78764],\n",
    "    edge_label=[64],\n",
    "    edge_label_index=[2, 64],\n",
    "    n_id=[10103],\n",
    "    e_id=[78764],\n",
    "    num_sampled_nodes=[3],\n",
    "    num_sampled_edges=[2],\n",
    "    input_id=[64]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([128, 2531, 7659], [4452, 74455])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.num_sampled_nodes, b.num_sampled_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.edge_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data_dict[\"train_loader\"]\n",
    "test_loader = data_dict[\"test_loader\"]\n",
    "val_loader = data_dict[\"val_loader\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Training\n",
    "\n",
    "\n",
    "Model based on [DSKReG: Differentiable Sampling on Knowledge Graph for\n",
    "Recommendation with Relational GNN](https://arxiv.org/pdf/2108.11883v1)\n",
    "\n",
    "\n",
    "[BPR: Bayesian Personalized Ranking](https://arxiv.org/pdf/1205.2618) loss used. This loss mostly optimize ranking of model's predictions. The main idea of this loss is to maximize the probability of user prefering observed item over an unobserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import softmax\n",
    "import torch.nn as nn\n",
    "import torch_scatter\n",
    "\n",
    "\n",
    "class DSKReG(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_relations: int,\n",
    "        num_classes: int,\n",
    "        top_k: int = 5,\n",
    "    ) -> None:\n",
    "        super(DSKReG, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.num_relations = num_relations\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.top_k = top_k\n",
    "\n",
    "        self.linear_rel = nn.Linear(hidden_dim * 2, 1, bias=True)\n",
    "        self.linear_agg = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "\n",
    "        self.relation_weight = nn.Parameter(torch.randn(hidden_dim))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type, user_emb, size=None):\n",
    "        self.propagate(\n",
    "            edge_index, x=x, edge_type=edge_type, user_emb=user_emb, size=size\n",
    "        )\n",
    "\n",
    "    def message(self, x_i, x_j, index, ptr, size_i):\n",
    "        pass\n",
    "\n",
    "    def aggregate(self, inputs, index, ptr=None, dim_size=None):\n",
    "        return torch_scatter.scatter(inputs, index, dim=0, reduce=\"mean\")\n",
    "\n",
    "    def rel_scores(self, relation_emb, neighbor_emb):\n",
    "        concat_emb = torch.cat([relation_emb, neighbor_emb], dim=-1)\n",
    "        return torch.softmax(self.linear_rel(concat_emb).squeeze(-1), dim=0)\n",
    "\n",
    "    def gumbel_softmax_sampling(self, relevance_score, index):\n",
    "        grouped_scores = softmax(relevance_score, index=index)\n",
    "\n",
    "        gumbel_noise = (\n",
    "            torch.rand_like(grouped_scores).log()\n",
    "            - torch.rand_like(grouped_scores).log()\n",
    "        )\n",
    "\n",
    "        softmax_logits = torch.softmax(\n",
    "            (torch.log(grouped_scores) + gumbel_noise) / self.tau, dim=0\n",
    "        )\n",
    "\n",
    "        _, top_k_indices = torch.topk(\n",
    "            softmax_logits, self.top_k, dim=0, largest=True, sorted=False, out=None\n",
    "        )\n",
    "\n",
    "        mask = torch.zeros_like(softmax_logits)\n",
    "        mask[top_k_indices] = 1.0\n",
    "\n",
    "        return mask * softmax_logits\n",
    "\n",
    "    def loss(self, user_emb, pos_item_emb, neg_item_emb, reg_lambda=0.001):\n",
    "        pos_scores = (user_emb * pos_item_emb).sum(dim=-1)\n",
    "        neg_scores = (user_emb * neg_item_emb).sum(dim=-1)\n",
    "\n",
    "        bpr_loss = -torch.log(torch.sigmoid(pos_scores - neg_scores)).mean()\n",
    "        l2_norm = (\n",
    "            user_emb.norm(2).pow(2)\n",
    "            + pos_item_emb.norm(2).pow(2)\n",
    "            + neg_item_emb.norm(2).pow(2)\n",
    "        )\n",
    "\n",
    "        return bpr_loss + reg_lambda * l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(edge_index, num_nodes, node_types, num_neg_samples):\n",
    "    existing_edges = set(map(tuple, edge_index.T.tolist()))\n",
    "    neg_edges = set()\n",
    "\n",
    "    while len(neg_edges) < num_neg_samples:\n",
    "        u = torch.randint(0, num_nodes, (1,)).item()\n",
    "        v = torch.randint(0, num_nodes, (1,)).item()\n",
    "\n",
    "        if node_types[u] == \"user\" and node_types[v] == \"anime\":\n",
    "            if (u, v) not in existing_edges and (v, u) not in existing_edges:\n",
    "                neg_edges.add((u, v))\n",
    "        elif node_types[u] == \"anime\" and node_types[v] == \"user\":\n",
    "            if (u, v) not in existing_edges and (v, u) not in existing_edges:\n",
    "                neg_edges.add((u, v))\n",
    "\n",
    "    return torch.tensor(list(neg_edges), dtype=torch.long).T\n",
    "\n",
    "\n",
    "def validate(model, dataloader, device: str = \"cpu\"):\n",
    "    loss_ = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            #loss = model.loss(user_emb, pos_item_emb, neg_item_emb)\n",
    "            #loss_ += loss.item()\n",
    "            loss += 1\n",
    "    return loss_\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_dataloader,\n",
    "    validation_dataloader,\n",
    "    test_dataloader,\n",
    "    num_epochs: int = 10,\n",
    "    device: str = \"cpu\",\n",
    "    reg_lambda: float = 0.001,\n",
    "):\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        validation_loss = 0\n",
    "        test_loss = 0\n",
    "\n",
    "        train_loader = tqdm(\n",
    "            enumerate(train_dataloader), desc=f\"Epoch {epoch + 1}/{num_epochs}\"\n",
    "        )\n",
    "\n",
    "        for batch in train_loader:\n",
    "            (\n",
    "                x,\n",
    "                edge_index,\n",
    "                edge_type,\n",
    "                edge_label,\n",
    "                edge_label_index,\n",
    "                n_id,\n",
    "                e_id,\n",
    "                num_sampled_nodes,\n",
    "                num_sampled_edges,\n",
    "                input_id,\n",
    "            ) = batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            num_neg_samples = \n",
    "            neg_edge_index = negative_sampling(\n",
    "                edge_index, num_nodes, node_types, num_neg_samples\n",
    "            )\n",
    "\n",
    "            pos_item_emb = user_emb[pos_edge_index[1]]\n",
    "            neg_item_emb = user_emb[neg_edge_index[1]]\n",
    "\n",
    "            loss = model.loss(user_emb, pos_item_emb, neg_item_emb)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            train_loader.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        validation_loss = validate(validation_dataloader)\n",
    "        test_loss = validate(test_dataloader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}, Training Loss: {epoch_loss / len(train_dataloader):.4f}\"\n",
    "        )\n",
    "        print(f\"Validation Loss: {validation_loss / len(validation_dataloader):.4f}\")\n",
    "        print(f\"Test Loss: {test_loss / len(test_dataloader):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
