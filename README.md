# Recomendational System using Graph Neural Networks with Pytorch Geometric for DSKReG implementation

## Table of Contents
- [Introduction](#introduction)
- [What is DSKReG](#what-is-dskreg)
- [Dataset Exploration](#dataset-exploration)
- [Dataset Preparation](#dataset-preparation)
    - [Features Preprocessing](#features-preprocessing)
    - [PyG Dataset Construction](#pyg-dataset-construction)
- [DSKReG Model](#dskreg-model)
    - [Differentiable Sampling](#differentiable-sampling)
    - [Preference Aware Aggregation](#preference-aware-aggregation)
    - [Creating Loss](#creating-loss)
    - [Model Training](#model-training)

## Introduction

Recomendation Systems or RecSys play a crucial role in our world - mostly each digital product uses it in some way. You defenetly had experiance of interacting with RecSys: your moives and snacks recomendation generated by RecSys. chances are you’ve experienced both its successes and failures. For example, a recommendation engine might suggest a genre of movies you dislike (e.g., horror films, when you only watch sci-fi). Therefore, creation of great RecSys become an essential step for businesses in today’s digital world.

RecSys algorithms (or models) have both: advanteges and disadvantages. With _Graph Neural Networks (GNNs)_ emerged a powerful approach to recommendation systems, with capability of modeling complex relationships and dependencies between users, items, and their attributes. DSKReG (Differentiable Sampling on Knowledge Graph for Recommendation with Relational GNN) is one such advanced method that uses Graph Neural Networks to improve recommendations, and we’ll see how we can implement it using PyTorch Geometric.

You can see our code in [this jupyter notebook](link)

For a more detailed explanation, you can check out the paper [Differentiable Sampling on Knowledge Graph for Recommendation with Relational GNN](https://arxiv.org/pdf/2108.11883v1).

## What is DSKReG

DSKReG is an approach to recommendation systems that integrates graph-based models with relational data. It specifically focuses on knowledge graphs (KG), which represent relationships between different entities (such as users, items, and their interactions). The model utilizes Graph Neural Networks (GNNs), which are designed to learn representations of graph-structured data, to process the relationships between entities and generate recommendations.

The main advantage of DSKReG lies in its ability to differentiate sampling within knowledge graphs. By doing so, it can handle complex relationships between users and items, thereby improving the quality of the recommendations.

The authors of the paper highlight two key features of DSKReG:

1. Differentiable Sampling: This means the model can be trained end-to-end, which is not possible with traditional graph-based models that require non-differentiable components.
2. Relational GNN: The model uses a relational version of GNNs, meaning it can take into account various types of relations in the graph, which is crucial for recommendation tasks that involve multiple different types of interactions.

## Dataset Exploration

For our demonstration, we’ve chosen the [anime dataset](https://www.kaggle.com/datasets/CooperUnion/anime-recommendations-database), a popular dataset in recommendation system research. This dataset contains information about various anime titles, users who rated them, and their corresponding ratings.

Dataset has following usefull information:

- User-Anime Interaction information: table of user_id, anime_id and rating from 0 to 10 if user rated the anime title or -1 otherwise
- Anime Information: genres and type (Movie, TV)

## Dataset Preparation

Import all needed packages and read the dataset:

```python
import torch_geometric
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader, LinkNeighborLoader
import torch_geometric.transforms as T
import torch

import networkx as nx

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pickle

# Load dataset
anime = pd.read_csv("../data/anime.csv")
rating = pd.read_csv("../data/rating.csv")
anime['genre'] = anime['genre'].str.split(', ')

# Remove NaN values from both DataFrames
nan_anime_id = anime.loc[anime.isna().any(axis=1), 'anime_id'].values
anime = anime.dropna()
rating = rating.loc[~rating.loc[:, 'anime_id'].isin(nan_anime_id), :]

# Remove animes that are in `rating`, but not exist in `anime`
rating = rating.loc[rating.loc[:, 'anime_id'].isin(anime.loc[:, 'anime_id'].unique()), :]
```

Now for convinience we will map id's of users and anime titles

```python
unique_anime_id = pd.DataFrame(data={
    'anime_id': anime.loc[:, 'anime_id'],
    'mapped_id': pd.RangeIndex(len(anime)),
})
mapped_anime_id_rating = pd.merge(unique_anime_id, anime, on='anime_id').loc[:, ['anime_id', 'mapped_id', 'rating']]


# Perform merge to obtain the edges from users and animes:
ratings_user_id = pd.merge(rating.loc[:, 'user_id'], unique_user_id,
                            left_on='user_id', right_on='user_id', how='left')
ratings_user_id = torch.from_numpy(ratings_user_id['mapped_id'].values)
ratings_anime_id = pd.merge(rating.loc[:, 'anime_id'], unique_anime_id,
                            left_on='anime_id', right_on='anime_id', how='left')
```

Now we finished with intial dataset preparation, so we can preprocess all the features into edge indexes for our PyG Dataset.

### Features preprocessing

Firstly, we need to construct `edge_index` for our PyG Dataset.

```Python
edge_index_user_to_anime = torch.stack([ratings_user_id, ratings_anime_id], dim=0)
```

Encode the genres and types for each anime using [One-Hot Encoding or pd.get_dummies()](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html):

```Python
types = anime["type"].unique()
genres = anime["genre"].explode().unique()

print(f"Unique types: {len(types)}")
print(f"Unique genres: {len(genres)}")
# Unique types: 6
# Unique genres: 43

type2id = {t:i for i, t in enumerate(types)}
id2type = {i:t for i, t in enumerate(types)}

genre2id = {g:i for i, g in enumerate(genres)}
id2genre = {i:g for i, g in enumerate(genres)}
```

Split the genres by `|`:

```Python
anime['genre'] = anime.loc[:, 'genre'].apply(func=lambda x: '|'.join(x))
# Result: `Action|Adventure|Comedy|Drama|Sci-Fi|Space`
```

Use `pd.get_dummies()` to encode types and genres:
```Python
types_encoded = anime.loc[:, 'type'].str.get_dummies()
genres_encoded = anime.loc[:, 'genre'].str.get_dummies('|')
```

Now we have 2 Dataframes, where each column can have only one out of two values:
`1` - Present in the anime at row index;
`0` - Is not present in the anime at row index.

Now we define a helper function for mapping anime id to the genre or type:

```Python
def edge_index_anime_to_enc(mapped_df: pd.DataFrame) -> pd.DataFrame:

    # Initialize an empty list to store the results
    result_data = []

    # Iterate through each row of the dataframe
    for index, row in mapped_df.iterrows():
        mapped_id = row['mapped_id']
        # Iterate through each genre column
        for genre_index, genre in enumerate(mapped_df.columns[:-1]):  # Exclude the last column 'mapped_id'
            if row[genre] == 1:
                result_data.append([mapped_id, genre_index])

    # Create a new dataframe from the result data
    result_df = pd.DataFrame(result_data, columns=['mapped_id', 'genre'])

    return result_df
```

And apply it on the `genres_encoded`:

```Python
genres_encoded['mapped_id'] = mapped_anime_id_rating.loc[:, 'mapped_id']
edge_index_anime_to_genre = edge_index_anime_to_enc(genres_encoded)
# The `edge_index_anime_to_genre` will look like this:
#
# mapped_id     genre
# 0             0
# 0             1
# 0             3
# 0             6
# 0             28
# etc.
```

Transform `edge_index_anime_to_genre` to `torch.Tensor()`:

```Python
edge_index_anime_to_genre = torch.from_numpy(edge_index_anime_to_genre.T.values).to(torch.int64)

print(edge_index_anime_to_genre.shape)
# torch.Size([2, 35594])
```

Do the same with `types_encoded`:

```Python
types_encoded['mapped_id'] = mapped_anime_id_rating.loc[:, 'mapped_id']
edge_index_anime_to_type = edge_index_anime_to_enc(types_encoded)
# The `edge_index_anime_to_type` will look like this:
#
# mapped_id     type
# 0             5
# 1             0
# 2             5
# 3             5
# 4             5
# etc.

edge_index_anime_to_type = torch.from_numpy(edge_index_anime_to_type.T.values).to(torch.int64)

print(edge_index_anime_to_type.shape)
# torch.Size([2, 12017])
```

### PyG Dataset construction

In our case we will use [`HeteroData`](https://pytorch-geometric.readthedocs.io/en/2.5.0/generated/torch_geometric.data.HeteroData.html):

```Python
# Define Heterogeneous graph
data = HeteroData()

# Add users and anime to our graph
data['user'].node_id = torch.arange(len(unique_user_id))
data['anime'].node_id = torch.arange(len(edge_index_user_to_anime[1].unique()))

# Add genres and types to our graph
data['genre'].node_id = torch.arange(len(genres))
data['type'].node_id = torch.arange(len(types))

# Add node features for 'anime'
data['anime'].mean_rating = torch.from_numpy(mapped_anime_id_rating.loc[:, 'rating'].values).to(torch.float64)

# Add edges and edge attributes
data['user', 'rates', 'anime'].edge_index = edge_index_user_to_anime
data['user', 'rates', 'anime'].rating = torch.from_numpy(rating.loc[:, 'rating'].values).to(torch.int64)

data['anime', 'has_genres', 'genre'].edge_index = edge_index_anime_to_genre
data['anime', 'has_type', 'type'].edge_index = edge_index_anime_to_type

# Transform data to undirected graph
data = T.ToUndirected()(data)
```

Our `data` will have the following structure:
```Python
HeteroData(
  user={ node_id=[73515] },
  anime={
    node_id=[11162],
    mean_rating=[12017],
  },
  genre={ node_id=[43] },
  type={ node_id=[6] },
  (user, rates, anime)={
    edge_index=[2, 7813611],
    rating=[7813611],
  },
  (anime, has_genres, genre)={ edge_index=[2, 35594] },
  (anime, has_type, type)={ edge_index=[2, 12017] },
  (anime, rev_rates, user)={
    edge_index=[2, 7813611],
    rating=[7813611],
  },
  (genre, rev_has_genres, anime)={ edge_index=[2, 35594] },
  (type, rev_has_type, anime)={ edge_index=[2, 12017] }
)
```

Now, we will split `data` in 3 subgraphs via [`RandomLinkSplit()`](https://pytorch-geometric.readthedocs.io/en/2.5.2/generated/torch_geometric.transforms.RandomLinkSplit.html):

```Python
transform = T.RandomLinkSplit(
    num_val=0.1,
    num_test=0.1,
    is_undirected=True,
    disjoint_train_ratio=0.0,
    neg_sampling_ratio=2,
    add_negative_train_samples=False,
    edge_types=[("user", "rates", "anime")],
    rev_edge_types=[("anime", "rev_rates", "user")],
)

train_data, val_data, test_data = transform(data)
```

Some statistics about `train_data`:

- Number of user nodes: 73515
- Number of anime nodes: 11162
- Number of 'rates' edges: 6298500
- Number of 'rev_rates' edges: 6298500
- Maximum possible 'rates' edges: 820574430
- Maximum possible 'rev_rates' edges: 820574430
- `rates` edge density: 0.007676
- `rev_rates` edge density: 0.007676

As we can see, our graph is quite sparse, so there is a need for careful [`DataLoader`](https://pytorch-geometric.readthedocs.io/en/2.5.2/modules/loader.html) selection.

In our case, we will try to use [`LinkNeighborLoader`](https://pytorch-geometric.readthedocs.io/en/stable/modules/loader.html#torch_geometric.loader.LinkNeighborLoader).

This loader first selects a sample of edges from the set of input edges edge_label_index (which may or not be edges in the original graph) and then constructs a subgraph from all the nodes present in this list by sampling num_neighbors neighbors in each iteration.

```Python
link_loader = LinkNeighborLoader(
    data=train_data,
    num_neighbors={
        ('user', 'rates', 'anime'): [5, 3],
        ('anime', 'rev_rates', 'user'): [5, 3],
        ('anime', 'has_genres', 'genre'): [5, 3],
        ('genre', 'rev_has_genres', 'anime'): [5, 3],
        ('anime', 'has_type', 'type'): [5, 3],
        ('type', 'rev_has_type', 'anime'): [5, 3]
    },
    edge_label_index=('user', 'rates', 'anime'),
    edge_label=torch.arange(train_data['user', 'rates', 'anime'].edge_index.size(1)), 
    batch_size=32,
)
```

## DSKReG Model

DSKReG model consist of two main parts: Differentiable Sampling and Preference Aware Aggregation.
PyG greatly help us. Let's declare our model:

```python
from torch_geometric.nn.conv import MessagePassing
from torch_geometric.utils import softmax
import torch.nn as nn
import torch_scatter


class DSKReG(MessagePassing):
    def __init__(
        self,
        input_dim: int,
        hidden_dim: int,
        num_relations: int,
        num_classes: int,
        top_k: int = 5,
    ) -> None:
        super(DSKReG, self).__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim

        self.num_relations = num_relations
        self.num_classes = num_classes

        self.top_k = top_k

        self.linear_rel = nn.Linear(hidden_dim * 2, 1, bias=True)
        self.linear_agg = nn.Linear(hidden_dim, hidden_dim, bias=True)

        self.relation_weight = nn.Parameter(torch.randn(hidden_dim))

```

MessagePassing is ...
torch_scatter ...

### Differentiable Sampling

```python
class DSKReG(MessagePassing):
    ...

    def gumbel_softmax_sampling(self, relevance_score, index):
        grouped_scores = softmax(relevance_score, index=index)

        # Gumbel noise creation
        gumbel_noise = (
            torch.rand_like(grouped_scores).log() - torch.rand_like(grouped_scores).log()
        )

        softmax_logits = torch.softmax(
            (torch.log(grouped_scores) + gumbel_noise) / self.tau,
            dim=0
        )

        _, top_k_indices = torch.topk(
            softmax_logits,
            self.top_k,
            dim=0,
            largest=True,
            sorted=False,
            out=None
        )

        mask = torch.zeros_like(softmax_logits)
        mask[top_k_indices] = 1.0

        return mask * softmax_logits
```

### Preference Aware Aggregation

The model will look at User-Anime interactions. It will learn how users interact with anime titles by aggregating information from the items they have interacted with. For example, it a user has watched several anime titles, the model will combine the features of those titles to create a representation of the user in "item-space”

```python
class DSKReG(MessagePassing):
    ...

    def rel_scores(self, relation_emb, neighbor_emb):
        concat_emb = torch.cat([relation_emb, neighbor_emb], dim=-1)
        return torch.softmax(self.linear_rel(concat_emb).squeeze(-1), dim=0)

    def gumbel_softmax_sampling(self, relevance_score, index):
        grouped_scores = softmax(relevance_score, index=index)

        # Gumbel noise creation
        gumbel_noise = (
            torch.rand_like(grouped_scores).log() - torch.rand_like(grouped_scores).log()
        )

        softmax_logits = torch.softmax(
            (torch.log(grouped_scores) + gumbel_noise) / self.tau,
            dim=0
        )

        _, top_k_indices = torch.topk(
            softmax_logits,
            self.top_k,
            dim=0,
            largest=True,
            sorted=False,
            out=None
        )

        mask = torch.zeros_like(softmax_logits)
        mask[top_k_indices] = 1.0

        return mask * softmax_logits
```

### Creating Loss

We use the dot-product to generate the preference score of user $u$ to item $i$ with the inferred user/item embeddings $\hat{\mathbf{e}}_{u}$ and $\hat{\mathbf{e}}_{i}$, respectively.

The prediction is calculated as follows: $\hat{y}_{ui} = \sigma(\hat{\mathbf{e}}_{u}^{\intercal} \hat{\mathbf{e}}\_{i}) $

DSKReG uses BPR loss to optimize top-N recommendation, which is defined as follows:

$ \mathcal{L}_{bpr} = \sum_{(u,l,j) \in \mathcal{D}} -\text{log} \sigma \left( \hat{y}(u,l) - \hat{y}(u,j) \right) + \lambda ||\Theta||^{2}\_{2} $

```python
class DSKReG(MessagePassing):
    ...
    def loss(self, user_emb, pos_item_emb, neg_item_emb, reg_lambda=0.001):
        pos_scores = (user_emb * pos_item_emb).sum(dim=-1)
        neg_scores = (user_emb * neg_item_emb).sum(dim=-1)

        bpr_loss = -torch.log(torch.sigmoid(pos_scores - neg_scores)).mean()
        l2_norm = (
            user_emb.norm(2).pow(2)
            + pos_item_emb.norm(2).pow(2)
            + neg_item_emb.norm(2).pow(2)
        )

        return bpr_loss + reg_lambda * l2_norm
```

For loss calculation during training model gets negative examples, witch are items user $u$ has never interacted with.

### Model Training
